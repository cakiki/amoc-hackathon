{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-Anonymizing Text by Fingerprinting Language Generation (https://arxiv.org/abs/2006.09615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e612a2518e4aed8c236440639e4e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6198436cb2cb435cbd2732a7370c5a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cdde02a1e242f58df0b8061ec38d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f89dc9a9b24e7b990ca37d96a24cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFAutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly adapted from https://github.com/huggingface/transformers/blob/master/src/transformers/generation_tf_utils.py\n",
    "\n",
    "def return_nucleus_size(logits, top_p=0.8, print_greedy=False):\n",
    "    logits = tf.squeeze(logits)\n",
    "    indices_sorted = tf.argsort(logits, direction=\"DESCENDING\")\n",
    "    logits_sorted = tf.gather(logits, indices_sorted)\n",
    "    probs_sorted = tf.nn.softmax(logits_sorted, axis=-1)\n",
    "    cumulative_probs_sorted = tf.math.cumsum(probs_sorted, axis=-1)\n",
    "    if print_greedy:\n",
    "        print(tokenizer.decode(indices_sorted[0]))\n",
    "#     cutoff_index = tf.argmax(cumulative_probs_sorted>top_p)\n",
    "#     indices_to_filter = indices_sorted[cutoff_index+1:]\n",
    "#     indices_to_filter = tf.expand_dims(indices_to_filter, axis=1)\n",
    "#     filters = tf.fill([len(indices_to_filter)],-np.inf)\n",
    "#     filtered_logits = tf.tensor_scatter_nd_update(logits, indices_to_filter, filters)    \n",
    "    return 1 + tf.argmax(cumulative_probs_sorted>top_p).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      " had\n",
      " a\n",
      " man\n",
      " who\n",
      " beautiful\n",
      " that\n",
      " could\n",
      " barely\n",
      " see\n",
      " him\n",
      " his\n",
      " eyes\n",
      ".\n",
      "CPU times: user 899 ms, sys: 24.3 ms, total: 923 ms\n",
      "Wall time: 906 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10855, 287, 781, 2755, 924, 1251, 10, 73, 137, 118, 26, 4, 7, 23]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text = \"I once saw a man so tall I could not look into his eyes\"\n",
    "tokenized = tokenizer(text)\n",
    "input_ids = tokenized['input_ids']\n",
    "# for input_id in input_ids:\n",
    "#     print(tokenizer.prepare_for_model([input_id]))\n",
    "\n",
    "past = None\n",
    "NSS = []\n",
    "for input_id in input_ids:\n",
    "    inputs = tokenizer.prepare_for_model([input_id], return_tensors='tf')['input_ids']\n",
    "    output = model({'input_ids': inputs, 'past':past})\n",
    "    logits = output.logits\n",
    "    past = output.past_key_values\n",
    "    NSS.append(return_nucleus_size(logits, top_p=0.9, print_greedy=True))\n",
    "NSS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.prepare_for_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

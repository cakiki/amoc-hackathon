{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-Anonymizing Text by Fingerprinting Language Generation (https://arxiv.org/abs/2006.09615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e612a2518e4aed8c236440639e4e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6198436cb2cb435cbd2732a7370c5a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cdde02a1e242f58df0b8061ec38d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f89dc9a9b24e7b990ca37d96a24cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFAutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly adapted from https://github.com/huggingface/transformers/blob/master/src/transformers/generation_tf_utils.py\n",
    "\n",
    "def return_nucleus_size(logits, top_p=0.8, print_greedy=False):\n",
    "    logits = tf.squeeze(logits)\n",
    "    indices_sorted = tf.argsort(logits, direction=\"DESCENDING\")\n",
    "    logits_sorted = tf.gather(logits, indices_sorted)\n",
    "    probs_sorted = tf.nn.softmax(logits_sorted, axis=-1)\n",
    "    cumulative_probs_sorted = tf.math.cumsum(probs_sorted, axis=-1)\n",
    "    if print_greedy:\n",
    "        print(tokenizer.decode(indices_sorted[0]))\n",
    "#     cutoff_index = tf.argmax(cumulative_probs_sorted>top_p)\n",
    "#     indices_to_filter = indices_sorted[cutoff_index+1:]\n",
    "#     indices_to_filter = tf.expand_dims(indices_to_filter, axis=1)\n",
    "#     filters = tf.fill([len(indices_to_filter)],-np.inf)\n",
    "#     filtered_logits = tf.tensor_scatter_nd_update(logits, indices_to_filter, filters)    \n",
    "    return 1 + tf.argmax(cumulative_probs_sorted>top_p).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is\n",
      " a\n",
      " example\n",
      " of\n",
      " that\n",
      "CPU times: user 337 ms, sys: 8.95 ms, total: 346 ms\n",
      "Wall time: 339 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3485, 80, 774, 1, 113]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text = \"This is another example text\"\n",
    "tokenized = tokenizer(text)\n",
    "input_ids = tokenized['input_ids']\n",
    "# for input_id in input_ids:\n",
    "#     print(tokenizer.prepare_for_model([input_id]))\n",
    "\n",
    "past = None\n",
    "NSS = []\n",
    "for input_id in input_ids:\n",
    "    inputs = tokenizer.prepare_for_model([input_id], return_tensors='tf')['input_ids']\n",
    "    output = model({'input_ids': inputs, 'past':past})\n",
    "    logits = output.logits\n",
    "    past = output.past_key_values\n",
    "    NSS.append(return_nucleus_size(logits, print_greedy=True))\n",
    "NSS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3485, 80, 774, 1, 113]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.prepare_for_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

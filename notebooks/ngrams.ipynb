{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Image\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 stopwords from NLTK\n",
      "Loaded 326 stopwords from SPACY\n",
      "Loaded 318 stopwords from SKLEARN\n",
      "----------------------------------\n",
      "409 combined stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOPWORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "SKLEARN_STOPWORDS = set(SKLEARN_STOPWORDS)\n",
    "nltk.download('stopwords')\n",
    "NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "print(f'Loaded {len(NLTK_STOPWORDS)} stopwords from NLTK')\n",
    "print(f'Loaded {len(SPACY_STOPWORDS)} stopwords from SPACY')\n",
    "print(f'Loaded {len(SKLEARN_STOPWORDS)} stopwords from SKLEARN')\n",
    "stop_words = list(set.union(*[SKLEARN_STOPWORDS, SPACY_STOPWORDS, NLTK_STOPWORDS]))\n",
    "print('----------------------------------')\n",
    "print(f'{len(stop_words)} combined stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 3.44 s, total: 15.1 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_parquet('../data/base.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = df['body'].values\n",
    "corpus = [doc.lower() for doc in corpus]\n",
    "corpus = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "corpus = [[token for token in doc if (not token.isnumeric() and len(token) > 1)] for doc in corpus]\n",
    "corpus= [[token for token in doc if (not token in stop_words)] for doc in corpus]\n",
    "bigram = Phrases(corpus, min_count=1)\n",
    "trigram = Phrases(bigram[corpus], min_count=1)\n",
    "fourgram = Phrases(trigram[corpus], min_count=1)\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "fourgrams = []\n",
    "for doc in corpus:\n",
    "    b = [b for b in bigram[doc] if b.count('_') == 1]\n",
    "    t = [t for t in trigram[bigram[doc]] if t.count('_') == 2]\n",
    "    f = [f for f in fourgram[trigram[bigram[doc]]] if f.count('_') == 3]\n",
    "    bigrams.extend(b)\n",
    "    trigrams.extend(t)\n",
    "    fourgrams.extend(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
